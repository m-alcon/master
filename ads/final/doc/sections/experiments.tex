\section{Experiments}

Since I do not know how to perform rotations in a $k$-dimensional space (with $k > 3$) or creating the \textit{Householder} matrices, again, in a $k$-dimensional space, I was not able to implement \textit{NKD-tree} nor \textit{PKD-tree}. Furthermore, some formulas related to their implementation are not complete in the paper. I also made some research on how to do it, but I did not achieve anything. So, my experiments are focused on comparing the basic \textit{KD-tree} with the \textit{RKD-tree} proposed in the paper. Before starting with the experiments, I implemented in \textit{C++} both \textit{KD-tree} and \textit{RKD-tree}.

\subsection{\textit{KD-tree} implementation}
I only implemented the required operations of \textit{KD-trees} to make the experiments, which are construction and nearest neighbor search (\textit{NNS}). I based my implementation in \cite{wiki}, but I made the following changes.
\begin{itemize}
    \item \textbf{Construction.} To select the pivot among a point list, in each recursion step, I used the \textit{Quick Select} algorithm. Moreover, as the authors did in the paper, I implemented the random selection of the axis that is going to be used to create the splitting plane among a subset of the axis with high variance. In order to use less space, instead of saving one point in each node of the tree, I saved the index of the point's position within the main vector.
    \item \textbf{NNS.} To know if the other side of the splitting hyperplane crosses the hypersphere around the search point (with radius equal to the current nearest distance), I followed what is explained in \cite{nns}. I also created a new type of search, the \textit{limited search}, which, given a positive integer $n$, it limits the search to $n$ nodes.
\end{itemize}

\subsection{\textit{RKD-tree} implementation}
For the \textit{RKD-tree}, I followed what is explained in the paper. In this case I want to explain what I did in detail.
\begin{itemize}
    \item \textbf{Construction.} Given $n$, $m$, $k$ and a vector of points $V$, first the algorithm computes the top $k$ axis with highest variance, and then it generates the vector of its indices (I will refer to this vector as $HV$). Finally it creates $m$ \textit{KD-trees} as explained before. In order to create the trees without using a lot more of space, it passes to them a pointer to $V$ and to $HV$. The algorithm also stores $n$ as the maximum number of nodes that the search algorithm can visit. At this moment, we have $m$ \textit{KD-trees} with different structure, ready to be explored.
    \item \textbf{NNS.} Given a point $p$ that we want to search, first, for the root of each tree, the algorithm computes the distance between the root's point and $p$. It stores each root together with the computed distance inside a \textit{priority queue}. Then, while $n > 0$, while the queue is not empty and the algorithm does not find the exact point $p$, it takes the top element $e$ of the queue, it updates the closest point and its distance if $e$'s point is the closest one to $p$ visited so far. After that, it computes the distance between $p$ and each of $e$'s children, if possible, and add them to the queue. At the end of the algorithm, we have the closest point to $p$ that the algorithm can find visiting at most $n$ nodes of all the trees.
\end{itemize}