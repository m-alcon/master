\section{Algorithms}
\subsection{EM}
Expectation-maximization algorithm (EM) \cite{em,wiki_em} is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.

\subsection{DBSCAN}
Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm. Specifically, it is a density-based clustering non-parametric algorithm. So, given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).

Given $\epsilon$ and $minPts$, the DBSCAN algorithm can be abstracted into the following steps:
\begin{enumerate}
    \item Find the points in the $\epsilon$ neighborhood of every point, and identify the core points with more than $minPts$ neighbors.
    \item Find the connected components of core points on the neighbor graph, ignoring all non-core points.
    \item Assign each non-core point to a nearby cluster if the cluster is an  $\epsilon$ neighbor, otherwise assign it to noise.
\end{enumerate}