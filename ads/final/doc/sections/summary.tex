\section{Summary}
This paper focuses on improving the \textit{KD-tree} for indexing a large number of \textit{SIFT} (Scale invariant feature) and other types of image descriptors, which usually are large point vectors of high dimensionality.  In order to achieve this, the authors of the paper extended priority search to priority search among multiple trees, by creating them with the same data but with different structure. Also, the search of a point is simultaneously independent among the trees.

They proposed 3 different types of \textit{KD-trees} that have the properties described above.
\begin{itemize}
    \item \textit{\textbf{NKD-tree.}} Given $n$ and $m$, a \textit{NKD-tree} is a group of $m$ trees constructed with the same data but, as said before, with different structure. This difference is achieved by rotating each point of the data with an arbitrary rotation matrix $R$, which is unique for each of the trees. Related with the search of a point, the authors propose to search the point among multiple trees in the form of a concurrent search with a pooled priority queue. After descending each of the trees to find an initial nearest-neighbor candidate, the best candidate from all the trees is selected. They then pool the node ranking by using one queue to sort the nodes from all $m$ trees. As a result, all of them are searched in the order of their distance from the query point simultaneously.
    \item \textit{\textbf{RKD-tree.}} This kind of tree is almost the same than \textit{NKD-tree}, but it achieves the difference of structures in the construction stage by selecting a random dimension in which to subdivide the data, instead of selecting it sequentially. This dimension is selected among a few dimensions in which the data has a high variance.
    \item \textit{\textbf{PKD-tree.}} Again, the only difference between this kind of tree and the other two is how it achieves the difference of structure of all its trees. In this case, they align the data using \textit{PCA} (Principal Component Analysis) and then build multiple trees using rotations that fix the space spanned by the top principal axes.
\end{itemize}

Finally, the authors test the proposed trees. They conclude that \textit{NKD-trees} performs as well as \textit{RKD-trees} (success rate from $75\%$ to $88\%$), but that \textit{PKD-tree} applying random \textit{Householder}\footnote{Explained in the paper, not necessary for my experiments.} transformations to the data points, in order to preserve the \textit{PCA} subspace of appropriate dimension, leads to the highest success rate ($95\%$).