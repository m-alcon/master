\section{Experiments}
First, I tried to reproduce the experiment that is explained in the paper about \textit{RKD-trees} to check its result. This experiment compares the  expended time in searching one point in a \textit{KD-tree} and a \textit{RKD-tree}. I repeated it 100 times, in order to count how much times the \textit{RKD-tree} does not reach the closest point to the query point. I used the same parameters as the authors to construct the trees, which are the following ones:
\begin{itemize}
    \item A random vector of 128-dimensional points $V$ of size 20000, with real values between -10 and 10. In the case of the paper, points are normalized to unit length, but this just complicate things and is not relevant. It is also mentioned that tests with synthetic high-dimensional data led to even more dramatic improvements, up to 7-times diminished error rate with the \textit{NKD-tree}.
    \item A vector $HV$ with the indices of the top ($k$) axes with highest variance of $V$. The parameter $k$ is not given in the paper, so I set it to $32$ arbitrary.
    \item The number of trees $m$ is set to 6.
    \item The limitation of searched $n$ nodes is set to 1000;
\end{itemize}

The result of this experiment was bad. Although the searching time of the \textit{RKD-tree} was 10 times better than \textit{KD-tree} (\textit{RKD-tree} lasts 0.543s in average while \textit{KD-tree} lasts 5.114s), \textit{RKD-tree} could find the closest point only 12 times of the 100. Because of this, I tried to increase the accuracy of the \textit{RKD-tree}, so I made more experiments in order to reach the best possible accuracy with a good execution time. I tried different values of $n$, $m$ and $k$ to achieve the best combination.
